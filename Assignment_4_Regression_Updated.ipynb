{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4: Regression Aug 2020.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# Basic regression: Predict fuel efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHp3M9ZmrIxj"
      },
      "source": [
        "In a *regression* problem, we aim to predict the output of a continuous value, like a price or a probability. \n",
        "\n",
        "This notebook uses the classic [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) Dataset and builds a model to predict the fuel efficiency. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moB4tpEHxKB3"
      },
      "source": [
        "# Use seaborn for pairplot\n",
        "!pip install seaborn\n",
        "\n",
        "# Use some functions from tensorflow_docs\n",
        "!pip install git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xQKvCJ85kCQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)\n",
        "tf.random.set_seed(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz4HfsgRQUiV"
      },
      "source": [
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_72b0LCNbjx"
      },
      "source": [
        "## The Auto MPG dataset\n",
        "\n",
        "The dataset is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFh9ne3FZ-On"
      },
      "source": [
        "### Get the data\n",
        "First download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9kxxgzvzlyz"
      },
      "source": [
        "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
        "dataset_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nslsRLh7Zss4"
      },
      "source": [
        "Import it using pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "source": [
        "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "dataset = pd.read_csv(dataset_path, names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\" \", skipinitialspace=True)\n",
        "\n",
        "dataset.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MWuJTKEDM-f"
      },
      "source": [
        "### Clean the data\n",
        "\n",
        "The dataset contains a few unknown values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UPN0KBHa_WI"
      },
      "source": [
        "Drop those rows if they contain any unknown values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEJHhN65a2VV"
      },
      "source": [
        "# Write code to remove the unknown values\n",
        "dataset = dataset.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XKitwaH4v8h"
      },
      "source": [
        "The `\"Origin\"` column is really categorical, not numeric. So convert that to a one-hot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWNTD2QjBWFJ"
      },
      "source": [
        "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulXz4J7PAUzk"
      },
      "source": [
        "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
        "dataset.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuym4yvk76vU"
      },
      "source": [
        "### Split the data into train and test\n",
        "\n",
        "Now split the dataset into a training set and a test set.\n",
        "\n",
        "We will use the test set in the final evaluation of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn-IGhUE7_1H"
      },
      "source": [
        "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db7Auq1yXUvh"
      },
      "source": [
        "### Split features from labels\n",
        "\n",
        "Separate the target value, or \"label\", from the features. This label is the value that you will train the model to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2sluJdCW7jN"
      },
      "source": [
        "train_labels = train_dataset.pop('MPG')\n",
        "test_labels = test_dataset.pop('MPG')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRklxK5s388r"
      },
      "source": [
        "### Normalize the data\n",
        "\n",
        "Use z-score normalization for both datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPHR4oFf9hmC"
      },
      "source": [
        "# Write code here: To normalize both train and test datasets\n",
        "train_stats = train_dataset.describe()\n",
        "train_stats = train_stats.transpose()\n",
        "train_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5HSswdatAmB"
      },
      "source": [
        "def norm(x):\n",
        "  return (x - train_stats['mean']) / train_stats['std']\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmjdzxKzEu1-"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWtkIjhrZwa"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "(Q7) Build a `Sequential` model with two densely connected hidden layers (with 64 units and `relu` activation), and an output layer that returns a single, continuous value. The model building steps must be wrapped in a function, `build_model`. Use Adam optimizer with its default arguments and `mse` as the loss metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYaTahjitDOe"
      },
      "source": [
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "                            layers.Dense(64, activation='relu', input_shape = [len(train_dataset.keys())]),\n",
        "                            layers.Dense(64, activation = 'relu'),\n",
        "                            layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "  model.compile(loss ='mse', \n",
        "                optimizer=optimizer,\n",
        "                metrics = ['mae', 'mse'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGbPb-PHGbhs"
      },
      "source": [
        "# Build and compile your model in this cell.\n",
        "model = build_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-qWCsh6DlyH"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Train the model for 1000 epochs, and record the training and validation accuracy in the `history` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD7qHCmNIOY0"
      },
      "source": [
        "EPOCHS = 1000\n",
        "\n",
        "history = model.fit(\n",
        "  normed_train_data, train_labels,\n",
        "  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n",
        "  callbacks=[tfdocs.modeling.EpochDots()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl_yNr5n1kms"
      },
      "source": [
        "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyDMnBHhvrtd"
      },
      "source": [
        "Q. 8 Change the model architecture by increasing units of both hidden layers to 100. Train the model again with. What is the range of the training error after the 900th epoch of the training process? Try to think of why this is happening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeJhpJFQu4aR"
      },
      "source": [
        "def build_model2():\n",
        "  model2 = keras.Sequential([\n",
        "                            layers.Dense(100, activation='relu', input_shape = [len(train_dataset.keys())]),\n",
        "                            layers.Dense(100, activation = 'relu'),\n",
        "                            layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "  model2.compile(loss ='mse', \n",
        "                optimizer=optimizer,\n",
        "                metrics = ['mae','mse'])\n",
        "  return model2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdfiaEwmu8r7"
      },
      "source": [
        "model2 = build_model2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3ba5W_cv47N"
      },
      "source": [
        "EPOCHS = 1000\n",
        "\n",
        "history = model2.fit(\n",
        "  normed_train_data, train_labels,\n",
        "  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n",
        "  callbacks=[tfdocs.modeling.EpochDots()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBgP29TuwDVX"
      },
      "source": [
        "loss, mae, mse = model2.evaluate(normed_test_data, test_labels, verbose=2)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQHSzjKww4zo"
      },
      "source": [
        "Q.10 We will now change the loss from ​ mse​ to ​ mae​ . The loss on the test data after training the best model lies in the range:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRjwHg9-wUZU"
      },
      "source": [
        "def build_model3():\n",
        "  model3 = keras.Sequential([\n",
        "                            layers.Dense(100, activation='relu', input_shape = [len(train_dataset.keys())]),\n",
        "                            layers.Dense(100, activation = 'relu'),\n",
        "                            layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "  model3.compile(loss ='mae', \n",
        "                optimizer=optimizer,\n",
        "                metrics = ['mae','mse'])\n",
        "  return model3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHh0N_erxBmn"
      },
      "source": [
        "model3 = build_model3()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPyF0I9xxIVv"
      },
      "source": [
        "EPOCHS = 1000\n",
        "\n",
        "history = model3.fit(\n",
        "  normed_train_data, train_labels,\n",
        "  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n",
        "  callbacks=[tfdocs.modeling.EpochDots()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDNsTN2sxO2s"
      },
      "source": [
        "loss, mae, mse = model3.evaluate(normed_test_data, test_labels, verbose=2)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh7q-v-DxqRu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}